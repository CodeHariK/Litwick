{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0257f907",
   "metadata": {},
   "source": [
    "# Probability, Entropy, Cross-Entropy\n",
    "\n",
    "* [Artem:\n",
    "The Key Equation Behind Probability](https://www.youtube.com/watch?v=KHVR587oW8I)\n",
    "\n",
    "* [StatQuest: \n",
    "Neural Networks Part 6: Cross Entropy](https://www.youtube.com/watch?v=6ArSys5qHAU)\n",
    "* [StatQuest: Neural Networks Part 7: Cross Entropy Derivatives and Backpropagation](https://www.youtube.com/watch?v=xBEh66V9gZo)\n",
    "\n",
    "\n",
    "# Loss Functions\n",
    "\n",
    "* https://www.digitalocean.com/community/tutorials/pytorch-loss-functions\n",
    "* https://keras.io/api/losses/\n",
    "* https://docs.pytorch.org/docs/stable/nn.html#loss-functions\n",
    "\n",
    "Loss functions measure how far model predictions are from the true values. The model learns by minimizing this loss during training.\n",
    "\n",
    "$$\\text{Loss} = f(y_{true}, y_{pred})$$\n",
    "\n",
    "---\n",
    "\n",
    "## Entropy & Cross-Entropy: The Foundation\n",
    "\n",
    "### The Origin: Shannon's Information Theory (1948)\n",
    "\n",
    "Claude Shannon asked: **How do we measure information?**\n",
    "\n",
    "The key insight is that information is related to **surprise**. \"The sun rose today\" isn't informative—you expected it. \"A meteor hit New York\" is highly informative because it's unexpected.\n",
    "\n",
    "Shannon formalized this. The **information content** of an event with probability $p$ should satisfy:\n",
    "\n",
    "1. **Rare events carry more information** — $I(p)$ decreases as $p$ increases\n",
    "2. **Certain events carry no information** — $I(1) = 0$\n",
    "3. **Independent events add up** — $I(p_1 \\cdot p_2) = I(p_1) + I(p_2)$\n",
    "\n",
    "The **only function** satisfying all three is the logarithm:\n",
    "\n",
    "$$I(x) = -\\log(p(x)) = \\log\\left(\\frac{1}{p(x)}\\right)$$\n",
    "\n",
    "This is called **self-information** or **surprisal**, measured in **bits** (log base 2) or **nats** (natural log).\n",
    "\n",
    "| Event | Probability | Information |\n",
    "|-------|-------------|-------------|\n",
    "| Fair coin heads | $p = 0.5$ | $-\\log_2(0.5) = 1$ bit |\n",
    "| Dice rolls 6 | $p = 1/6$ | $-\\log_2(1/6) \\approx 2.58$ bits |\n",
    "| Certain event | $p = 1$ | $-\\log_2(1) = 0$ bits |\n",
    "\n",
    "---\n",
    "\n",
    "### Entropy: Average Surprise\n",
    "\n",
    "**Entropy** is the *expected* information—the average surprise when sampling from a distribution:\n",
    "\n",
    "$$H(P) = \\mathbb{E}_{x \\sim P}[I(x)] = -\\sum_{x} p(x) \\log p(x)$$\n",
    "\n",
    "**Intuition 1: Minimum bits to encode messages**\n",
    "\n",
    "Entropy answers: *What's the minimum average number of bits needed to encode symbols from this distribution?*\n",
    "\n",
    "| Distribution | Entropy | Interpretation |\n",
    "|--------------|---------|----------------|\n",
    "| Fair coin (50/50) | 1 bit | Maximum uncertainty |\n",
    "| Biased coin (99/1) | 0.08 bits | Almost certain |\n",
    "| Uniform over 8 items | 3 bits | $\\log_2(8) = 3$ |\n",
    "\n",
    "**Intuition 2: Uncertainty/Disorder**\n",
    "\n",
    "- **High entropy** = flat, uniform, maximum uncertainty\n",
    "- **Low entropy** = peaked, concentrated, predictable\n",
    "\n",
    "---\n",
    "\n",
    "### Cross-Entropy: Encoding with the Wrong Distribution\n",
    "\n",
    "Suppose the true distribution is $P$, but you design your encoding based on distribution $Q$. The **cross-entropy** is:\n",
    "\n",
    "$$H(P, Q) = -\\sum_{x} p(x) \\log q(x)$$\n",
    "\n",
    "This is the **expected bits needed to encode samples from $P$ using a code optimized for $Q$**.\n",
    "\n",
    "**The critical relationship:**\n",
    "\n",
    "$$H(P, Q) = H(P) + D_{KL}(P \\| Q)$$\n",
    "\n",
    "Where $D_{KL}$ is **KL-divergence** (the penalty for using the wrong distribution):\n",
    "\n",
    "$$D_{KL}(P \\| Q) = \\sum_{x} p(x) \\log \\frac{p(x)}{q(x)} \\geq 0$$\n",
    "\n",
    "Since $D_{KL} \\geq 0$, we have $H(P, Q) \\geq H(P)$, with equality only when $P = Q$.\n",
    "\n",
    "---\n",
    "\n",
    "### Why Cross-Entropy for Classification?\n",
    "\n",
    "In classification:\n",
    "- **True distribution $P$**: one-hot labels (e.g., $[0, 1, 0]$)\n",
    "- **Predicted distribution $Q$**: softmax outputs (e.g., $[0.1, 0.7, 0.2]$)\n",
    "\n",
    "Since $y$ is one-hot, cross-entropy simplifies to:\n",
    "\n",
    "$$\\mathcal{L} = -\\sum_{c} y_c \\log(\\hat{y}_c) = -\\log(\\hat{y}_{true})$$\n",
    "\n",
    "**Why it works:**\n",
    "\n",
    "1. **Penalizes confident wrong predictions harshly** — predict 0.01 for true class → loss = 4.6; predict 0.99 → loss = 0.01\n",
    "\n",
    "2. **Beautiful gradient** — for softmax + cross-entropy: $\\frac{\\partial \\mathcal{L}}{\\partial z_i} = \\hat{y}_i - y_i$\n",
    "\n",
    "3. **Maximum likelihood** — minimizing cross-entropy = maximizing log-likelihood\n",
    "\n",
    "---\n",
    "\n",
    "### Visual Summary\n",
    "\n",
    "```\n",
    "                    Information Theory\n",
    "                           │\n",
    "              ┌────────────┴────────────┐\n",
    "              │                         │\n",
    "         Self-Information           Entropy\n",
    "         I(x) = -log p(x)       H(P) = E[-log p(x)]\n",
    "         \"surprise of event\"    \"average surprise\"\n",
    "                                       │\n",
    "              ┌────────────────────────┴────────────────────────┐\n",
    "              │                                                 │\n",
    "        Cross-Entropy                                    KL-Divergence\n",
    "    H(P,Q) = E_P[-log q(x)]                          D_KL(P||Q) = H(P,Q) - H(P)\n",
    "    \"bits using wrong code\"                          \"penalty for wrong code\"\n",
    "              │\n",
    "              │\n",
    "    Classification Loss\n",
    "    L = -log(predicted prob of true class)\n",
    "```\n",
    "\n",
    "**The punchline:** Cross-entropy loss asks *how many extra bits do you waste by believing your model's distribution instead of the true distribution?* Minimizing it makes your model's beliefs match reality.\n",
    "\n",
    "---\n",
    "\n",
    "## Classification Losses\n",
    "\n",
    "### Cross-Entropy Loss (Log Loss)\n",
    "\n",
    "The most common loss for classification. Penalizes confident wrong predictions heavily.\n",
    "\n",
    "**Binary Cross-Entropy** — for binary classification (0 or 1):\n",
    "$$\\mathcal{L} = -\\frac{1}{N}\\sum_{i=1}^{N} \\left[ y_i \\log(\\hat{y}_i) + (1 - y_i) \\log(1 - \\hat{y}_i) \\right]$$\n",
    "\n",
    "**Categorical Cross-Entropy** — for multi-class (one-hot encoded labels):\n",
    "$$\\mathcal{L} = -\\sum_{c=1}^{C} y_c \\log(\\hat{y}_c)$$\n",
    "\n",
    "**Sparse Categorical Cross-Entropy** — mathematically identical, but labels are integers instead of one-hot vectors.\n",
    "\n",
    "| Variant | Label Format | Example (class 2 of 5) |\n",
    "|---------|--------------|------------------------|\n",
    "| Categorical | One-hot vector | `[0, 0, 1, 0, 0]` |\n",
    "| Sparse Categorical | Integer index | `2` |\n",
    "\n",
    "**Why \"sparse\"?** One-hot vectors are sparse (mostly zeros). Instead of storing/computing with sparse vectors, just store the index of the 1. For 1000 classes, you store 1 integer vs 1000 floats.\n",
    "\n",
    "**The math is the same:** Since the one-hot $y$ has only one non-zero entry at position $k$:\n",
    "$$\\mathcal{L} = -\\sum_{c=1}^{C} y_c \\log(\\hat{y}_c) = -\\log(\\hat{y}_k)$$\n",
    "\n",
    "Sparse version directly uses $k$ without creating the one-hot vector.\n",
    "\n",
    "| PyTorch | Keras | Use Case |\n",
    "|---------|-------|----------|\n",
    "| `nn.BCELoss()` | `BinaryCrossentropy()` | Binary (after sigmoid) |\n",
    "| `nn.BCEWithLogitsLoss()` | `BinaryCrossentropy(from_logits=True)` | Binary (raw logits) |\n",
    "| `nn.CrossEntropyLoss()` | `SparseCategoricalCrossentropy()` | Multi-class (integer labels) |\n",
    "| `nn.NLLLoss()` | `CategoricalCrossentropy()` | Multi-class (after log_softmax) |\n",
    "\n",
    "> **Note:** PyTorch's `CrossEntropyLoss` combines `log_softmax` + `NLLLoss` internally, so don't apply softmax to your model output!\n",
    "\n",
    "---\n",
    "\n",
    "### Focal Loss\n",
    "\n",
    "Addresses class imbalance by down-weighting easy examples.\n",
    "\n",
    "$$\\mathcal{L} = -\\alpha (1 - \\hat{y})^\\gamma \\log(\\hat{y})$$\n",
    "\n",
    "Where $\\gamma$ (focusing parameter) reduces loss for well-classified examples.\n",
    "\n",
    "| PyTorch | Keras |\n",
    "|---------|-------|\n",
    "| `torchvision.ops.sigmoid_focal_loss` | `BinaryFocalCrossentropy()` / `CategoricalFocalCrossentropy()` |\n",
    "\n",
    "---\n",
    "\n",
    "### Hinge Loss\n",
    "\n",
    "Used in SVMs and \"maximum-margin\" classification.\n",
    "\n",
    "$$\\mathcal{L} = \\max(0, 1 - y \\cdot \\hat{y})$$\n",
    "\n",
    "| PyTorch | Keras |\n",
    "|---------|-------|\n",
    "| `nn.HingeEmbeddingLoss()` | `Hinge()` / `SquaredHinge()` / `CategoricalHinge()` |\n",
    "\n",
    "---\n",
    "\n",
    "## Regression Losses\n",
    "\n",
    "### Mean Squared Error (MSE / L2 Loss)\n",
    "\n",
    "Penalizes large errors more due to squaring. Sensitive to outliers.\n",
    "\n",
    "$$\\mathcal{L} = \\frac{1}{N}\\sum_{i=1}^{N}(y_i - \\hat{y}_i)^2$$\n",
    "\n",
    "| PyTorch | Keras |\n",
    "|---------|-------|\n",
    "| `nn.MSELoss()` | `MeanSquaredError()` |\n",
    "\n",
    "---\n",
    "\n",
    "### Mean Absolute Error (MAE / L1 Loss)\n",
    "\n",
    "More robust to outliers than MSE.\n",
    "\n",
    "$$\\mathcal{L} = \\frac{1}{N}\\sum_{i=1}^{N}|y_i - \\hat{y}_i|$$\n",
    "\n",
    "| PyTorch | Keras |\n",
    "|---------|-------|\n",
    "| `nn.L1Loss()` | `MeanAbsoluteError()` |\n",
    "\n",
    "---\n",
    "\n",
    "### Huber Loss (Smooth L1)\n",
    "\n",
    "Combines MSE and MAE — quadratic for small errors, linear for large errors. Best of both worlds.\n",
    "\n",
    "$$\\mathcal{L} = \\begin{cases} \\frac{1}{2}(y - \\hat{y})^2 & \\text{if } |y - \\hat{y}| \\leq \\delta \\\\ \\delta|y - \\hat{y}| - \\frac{1}{2}\\delta^2 & \\text{otherwise} \\end{cases}$$\n",
    "\n",
    "| PyTorch | Keras |\n",
    "|---------|-------|\n",
    "| `nn.SmoothL1Loss()` / `nn.HuberLoss()` | `Huber()` |\n",
    "\n",
    "---\n",
    "\n",
    "### Cosine Similarity Loss\n",
    "\n",
    "Measures angle between vectors (ignores magnitude). Useful for embeddings.\n",
    "\n",
    "$$\\mathcal{L} = 1 - \\cos(\\theta) = 1 - \\frac{y \\cdot \\hat{y}}{||y|| \\cdot ||\\hat{y}||}$$\n",
    "\n",
    "| PyTorch | Keras |\n",
    "|---------|-------|\n",
    "| `nn.CosineEmbeddingLoss()` | `CosineSimilarity()` |\n",
    "\n",
    "---\n",
    "\n",
    "## Other Notable Losses\n",
    "\n",
    "| Loss | Use Case | PyTorch |\n",
    "|------|----------|---------|\n",
    "| **KL Divergence** | Distribution matching, VAEs | `nn.KLDivLoss()` |\n",
    "| **Triplet Loss** | Metric learning, face recognition | `nn.TripletMarginLoss()` |\n",
    "| **CTC Loss** | Sequence-to-sequence (speech, OCR) | `nn.CTCLoss()` |\n",
    "| **Dice Loss** | Image segmentation | Custom (Keras: `Dice()`) |\n",
    "\n",
    "---\n",
    "\n",
    "## Quick Reference\n",
    "\n",
    "| Task | Recommended Loss | Output Activation |\n",
    "|------|-----------------|-------------------|\n",
    "| Binary Classification | BCEWithLogitsLoss | None (raw logits) |\n",
    "| Multi-class Classification | CrossEntropyLoss | None (raw logits) |\n",
    "| Regression | MSELoss or L1Loss | None |\n",
    "| Regression (with outliers) | HuberLoss | None |\n",
    "| Segmentation | Dice + BCE | Sigmoid |\n",
    "\n",
    "**Sources:** [Keras Losses](https://keras.io/api/losses/) | [PyTorch Loss Functions](https://docs.pytorch.org/docs/stable/nn.html#loss-functions)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d869897f",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
