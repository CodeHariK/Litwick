{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8c433eaa",
   "metadata": {},
   "source": [
    "# Activation Functions\n",
    "\n",
    "## Softmax\n",
    "\n",
    "* [Neural Networks Part 5: ArgMax and SoftMax](https://www.youtube.com/watch?v=KpKog-L9veg)\n",
    "* [The SoftMax Derivative, Step-by-Step!!!](https://www.youtube.com/watch?v=M59JElEPgIg)\n",
    "\n",
    "Softmax converts a vector of raw scores (logits) into a **probability distribution**. Each output is between 0 and 1, and all outputs sum to 1.\n",
    "\n",
    "$$\\text{softmax}(z_i) = \\frac{e^{z_i}}{\\sum_{j=1}^{K} e^{z_j}}$$\n",
    "\n",
    "**Use case:** Multi-class classification (like MNIST with 10 digit classes). The output represents the probability of each class.\n",
    "\n",
    "**Example:** Input `[2.0, 1.0, 0.1]` → Output `[0.66, 0.24, 0.10]` (probabilities summing to 1)\n",
    "\n",
    "---\n",
    "\n",
    "## Sigmoid\n",
    "\n",
    "Sigmoid squashes any input value into the range **(0, 1)**.\n",
    "\n",
    "$$\\sigma(z) = \\frac{1}{1 + e^{-z}}$$\n",
    "\n",
    "**Use case:** Binary classification or independent multi-label classification (each output is treated independently).\n",
    "\n",
    "**Key difference from Softmax:**\n",
    "- **Sigmoid:** Each output is independent (can have multiple outputs > 0.5)\n",
    "- **Softmax:** Outputs are mutually exclusive (probabilities sum to 1)\n",
    "\n",
    "---\n",
    "\n",
    "## ReLU (Rectified Linear Unit)\n",
    "\n",
    "The most widely used activation in hidden layers. Outputs the input if positive, else 0.\n",
    "\n",
    "$$\\text{ReLU}(z) = \\max(0, z)$$\n",
    "\n",
    "**Pros:** Fast, avoids vanishing gradient for positive values  \n",
    "**Cons:** \"Dying ReLU\" — neurons can get stuck at 0 and stop learning\n",
    "\n",
    "---\n",
    "\n",
    "## Leaky ReLU\n",
    "\n",
    "Fixes dying ReLU by allowing a small gradient for negative inputs.\n",
    "\n",
    "$$\\text{LeakyReLU}(z) = \\begin{cases} z & \\text{if } z > 0 \\\\ \\alpha z & \\text{if } z \\leq 0 \\end{cases}$$\n",
    "\n",
    "Where $\\alpha$ is typically 0.01.\n",
    "\n",
    "---\n",
    "\n",
    "## Tanh (Hyperbolic Tangent)\n",
    "\n",
    "Squashes input to range **(-1, 1)**. Zero-centered unlike sigmoid.\n",
    "\n",
    "$$\\tanh(z) = \\frac{e^z - e^{-z}}{e^z + e^{-z}}$$\n",
    "\n",
    "**Use case:** Hidden layers when zero-centered output is preferred (e.g., RNNs).\n",
    "\n",
    "---\n",
    "\n",
    "## ELU (Exponential Linear Unit)\n",
    "\n",
    "Smooth alternative to ReLU with negative values that help push mean activations toward zero.\n",
    "\n",
    "$$\\text{ELU}(z) = \\begin{cases} z & \\text{if } z > 0 \\\\ \\alpha(e^z - 1) & \\text{if } z \\leq 0 \\end{cases}$$\n",
    "\n",
    "---\n",
    "\n",
    "## GELU (Gaussian Error Linear Unit)\n",
    "\n",
    "Used in Transformers (BERT, GPT). Smoothly gates values based on their magnitude.\n",
    "\n",
    "$$\\text{GELU}(z) = z \\cdot \\Phi(z)$$\n",
    "\n",
    "Where $\\Phi(z)$ is the CDF of standard normal distribution.\n",
    "\n",
    "---\n",
    "\n",
    "## Swish / SiLU\n",
    "\n",
    "Self-gated activation. Often outperforms ReLU in deep networks.\n",
    "\n",
    "$$\\text{Swish}(z) = z \\cdot \\sigma(z) = \\frac{z}{1 + e^{-z}}$$\n",
    "\n",
    "---\n",
    "\n",
    "## Summary Table\n",
    "\n",
    "| Function | Output Range | Zero-Centered | Best For |\n",
    "|----------|-------------|---------------|----------|\n",
    "| Sigmoid | (0, 1) | No | Output (binary) |\n",
    "| Softmax | (0, 1) | No | Output (multi-class) |\n",
    "| ReLU | [0, ∞) | No | Hidden layers (default) |\n",
    "| Leaky ReLU | (-∞, ∞) | No | Hidden layers |\n",
    "| Tanh | (-1, 1) | Yes | RNNs, hidden layers |\n",
    "| ELU | (-α, ∞) | ~Yes | Hidden layers |\n",
    "| GELU | (-0.17, ∞) | ~Yes | Transformers |\n",
    "| Swish | (-0.28, ∞) | ~Yes | Deep networks |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eaddc16c",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
