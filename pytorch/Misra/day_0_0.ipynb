{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f77161b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import YouTubeVideo\n",
    "\n",
    "YouTubeVideo(\"hfMk-kjRv4c\", width=640, height=360)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78a25533",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.animation import FuncAnimation\n",
    "from IPython.display import HTML\n",
    "\n",
    "# Create figure and axis\n",
    "fig, ax = plt.subplots()\n",
    "xdata, ydata = [], []\n",
    "ln, = ax.plot([], [], 'ro-')\n",
    "\n",
    "def init():\n",
    "    ax.set_xlim(0, 2*np.pi)\n",
    "    ax.set_ylim(-1.1, 1.1)\n",
    "    return ln,\n",
    "\n",
    "def update(frame):\n",
    "    xdata.append(frame)\n",
    "    ydata.append(np.sin(frame))\n",
    "    ln.set_data(xdata, ydata)\n",
    "    return ln,\n",
    "\n",
    "ani = FuncAnimation(fig, update, frames=np.linspace(0, 2*np.pi, 64),\n",
    "                    init_func=init, blit=True, interval=50)\n",
    "\n",
    "# Display in Jupyter\n",
    "HTML(ani.to_jshtml())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "084dd936",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.animation import FuncAnimation\n",
    "from IPython.display import HTML\n",
    "\n",
    "# Set random seeds for reproducibility\n",
    "torch.manual_seed(42)\n",
    "np.random.seed(42)\n",
    "\n",
    "# Generate random 2D data - two spiraling clusters\n",
    "n_points = 100\n",
    "noise = 0.2\n",
    "\n",
    "# Class 0: cluster centered around (-0.5, -0.5)\n",
    "X0 = np.random.randn(n_points, 2) * 0.4 + np.array([-0.5, -0.5])\n",
    "# Class 1: cluster centered around (0.5, 0.5)\n",
    "X1 = np.random.randn(n_points, 2) * 0.4 + np.array([0.5, 0.5])\n",
    "\n",
    "X = np.vstack([X0, X1]).astype(np.float32)\n",
    "y = np.array([0]*n_points + [1]*n_points).astype(np.float32)\n",
    "\n",
    "# Convert to PyTorch tensors\n",
    "X_tensor = torch.from_numpy(X)\n",
    "y_tensor = torch.from_numpy(y).unsqueeze(1)\n",
    "\n",
    "# Simple one-layer neural network\n",
    "class SimpleClassifier(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.linear = nn.Linear(2, 1)  # 2 inputs -> 1 output\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return torch.sigmoid(self.linear(x))\n",
    "\n",
    "model = SimpleClassifier()\n",
    "\n",
    "print(\"\\nModel Parameters:\")\n",
    "for name, param in model.named_parameters():\n",
    "    print(f\"  {name}: {param.shape} = {param.numel()} params\")\n",
    "total_params = sum(p.numel() for p in model.parameters())\n",
    "print(f\"\\nTotal: {total_params} parameters\")\n",
    "\n",
    "criterion = nn.BCELoss()\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=1.0)\n",
    "\n",
    "# Store decision boundaries at each epoch\n",
    "epochs = 100\n",
    "boundaries = []\n",
    "losses = []\n",
    "\n",
    "# Create mesh grid for decision boundary visualization\n",
    "xx, yy = np.meshgrid(np.linspace(-2, 2, 100), np.linspace(-2, 2, 100))\n",
    "grid = torch.from_numpy(np.c_[xx.ravel(), yy.ravel()].astype(np.float32))\n",
    "\n",
    "# Training loop - save state at each epoch\n",
    "for epoch in range(epochs):\n",
    "    # Forward pass\n",
    "    outputs = model(X_tensor)\n",
    "    loss = criterion(outputs, y_tensor)\n",
    "    \n",
    "    # Backward pass\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "    # Save decision boundary\n",
    "    with torch.no_grad():\n",
    "        Z = model(grid).numpy().reshape(xx.shape)\n",
    "        boundaries.append(Z.copy())\n",
    "        losses.append(loss.item())\n",
    "\n",
    "# Create animation\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 5))\n",
    "\n",
    "# Initialize plots\n",
    "contour = [None]\n",
    "scatter0 = ax1.scatter(X0[:, 0], X0[:, 1], c='red', edgecolors='white', s=60, label='Red')\n",
    "scatter1 = ax1.scatter(X1[:, 0], X1[:, 1], c='blue', edgecolors='white', s=60, label='Blue')\n",
    "ax1.set_xlim(-2, 2)\n",
    "ax1.set_ylim(-2, 2)\n",
    "ax1.set_xlabel('Feature 1')\n",
    "ax1.set_ylabel('Feature 2')\n",
    "ax1.legend()\n",
    "title = ax1.set_title('Epoch 0')\n",
    "\n",
    "# Loss plot\n",
    "loss_line, = ax2.plot([], [], 'b-', linewidth=2)\n",
    "ax2.set_xlim(0, epochs)\n",
    "ax2.set_ylim(0, max(losses) * 1.1)\n",
    "ax2.set_xlabel('Epoch')\n",
    "ax2.set_ylabel('Loss')\n",
    "ax2.set_title('Training Loss')\n",
    "ax2.grid(True, alpha=0.3)\n",
    "\n",
    "def init():\n",
    "    return []\n",
    "\n",
    "def update(frame):\n",
    "    # Clear previous contour\n",
    "    for c in ax1.collections[2:]:  # Keep scatter plots\n",
    "        c.remove()\n",
    "    \n",
    "    # Draw new decision boundary\n",
    "    ax1.contourf(xx, yy, boundaries[frame], levels=[0, 0.5, 1], \n",
    "\t\t\t\t\tcolors=['#ffcccc', '#ccccff'], alpha=0.6)\n",
    "    ax1.contour(xx, yy, boundaries[frame], levels=[0.5], \n",
    "                colors=['black'], linewidths=2)\n",
    "    \n",
    "    # Re-draw scatter on top\n",
    "    ax1.scatter(X0[:, 0], X0[:, 1], c='red', edgecolors='white', s=60, zorder=10)\n",
    "    ax1.scatter(X1[:, 0], X1[:, 1], c='blue', edgecolors='white', s=60, zorder=10)\n",
    "    \n",
    "    title.set_text(f'Epoch {frame + 1} | Loss: {losses[frame]:.4f}')\n",
    "    \n",
    "    # Update loss plot\n",
    "    loss_line.set_data(range(frame + 1), losses[:frame + 1])\n",
    "    \n",
    "    return []\n",
    "\n",
    "ani = FuncAnimation(fig, update, frames=epochs, init_func=init, \n",
    "                    interval=100, blit=False)\n",
    "plt.tight_layout()\n",
    "HTML(ani.to_jshtml())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b49588c5",
   "metadata": {},
   "source": [
    "## Single Output vs Two Output Classifier\n",
    "\n",
    "| Aspect | 1 Output (Above) | 2 Outputs (Below) |\n",
    "|--------|------------------|-------------------|\n",
    "| **Architecture** | 2 ‚Üí 1 | 2 ‚Üí 2 |\n",
    "| **Output Activation** | Sigmoid | Softmax |\n",
    "| **Loss Function** | BCELoss | CrossEntropyLoss |\n",
    "| **Output Meaning** | P(blue) | [P(red), P(blue)] |\n",
    "| **Parameters** | 3 (2 weights + 1 bias) | 6 (4 weights + 2 biases) |\n",
    "| **Decision** | output > 0.5 ‚Üí blue | argmax([out0, out1]) |\n",
    "\n",
    "**Why use 2 outputs?**\n",
    "- Scales naturally to multi-class (3+)\n",
    "- Each class has its own \"confidence score\"\n",
    "- Softmax ensures probabilities sum to 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2df549a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.animation import FuncAnimation\n",
    "from IPython.display import HTML\n",
    "\n",
    "# Set random seeds for reproducibility\n",
    "torch.manual_seed(42)\n",
    "np.random.seed(42)\n",
    "\n",
    "# Generate random 2D data - two clusters\n",
    "n_points = 100\n",
    "\n",
    "# Class 0 (Red): cluster centered around (-0.5, -0.5)\n",
    "X0 = np.random.randn(n_points, 2) * 0.4 + np.array([-0.5, -0.5])\n",
    "# Class 1 (Blue): cluster centered around (0.5, 0.5)\n",
    "X1 = np.random.randn(n_points, 2) * 0.4 + np.array([0.5, 0.5])\n",
    "\n",
    "X = np.vstack([X0, X1]).astype(np.float32)\n",
    "y = np.array([0]*n_points + [1]*n_points)  # Class labels: 0=red, 1=blue\n",
    "\n",
    "# Convert to PyTorch tensors\n",
    "X_tensor = torch.from_numpy(X)\n",
    "y_tensor = torch.from_numpy(y).long()  # CrossEntropyLoss needs long type\n",
    "\n",
    "# Two-output neural network\n",
    "class TwoOutputClassifier(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.linear = nn.Linear(2, 2)  # 2 inputs -> 2 outputs (one per class)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.linear(x)  # Raw logits (CrossEntropyLoss applies softmax internally)\n",
    "\n",
    "model = TwoOutputClassifier()\n",
    "\n",
    "print(\"Model Architecture: 2 inputs ‚Üí 2 outputs\")\n",
    "print(\"\\nModel Parameters:\")\n",
    "for name, param in model.named_parameters():\n",
    "    print(f\"  {name}: {param.shape} = {param.numel()} params\")\n",
    "total_params = sum(p.numel() for p in model.parameters())\n",
    "print(f\"\\nTotal: {total_params} parameter\")\n",
    "\n",
    "# CrossEntropyLoss = Softmax + NLLLoss (handles multi-class)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=1.0)\n",
    "\n",
    "# Store decision boundaries at each epoch\n",
    "epochs = 100\n",
    "boundaries = []\n",
    "losses = []\n",
    "\n",
    "# Create mesh grid for decision boundary visualization\n",
    "xx, yy = np.meshgrid(np.linspace(-2, 2, 100), np.linspace(-2, 2, 100))\n",
    "grid = torch.from_numpy(np.c_[xx.ravel(), yy.ravel()].astype(np.float32))\n",
    "\n",
    "# Training loop - save state at each epoch\n",
    "for epoch in range(epochs):\n",
    "    # Forward pass\n",
    "    outputs = model(X_tensor)  # Shape: [200, 2]\n",
    "    loss = criterion(outputs, y_tensor)\n",
    "    \n",
    "    # Backward pass\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "    # Save decision boundary (probability of class 1 / blue)\n",
    "    with torch.no_grad():\n",
    "        logits = model(grid)\n",
    "        probs = torch.softmax(logits, dim=1)  # Convert to probabilities\n",
    "        Z = probs[:, 1].numpy().reshape(xx.shape)  # P(blue)\n",
    "        boundaries.append(Z.copy())\n",
    "        losses.append(loss.item())\n",
    "\n",
    "# Create animation\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 5))\n",
    "fig.suptitle('Two-Output Classifier (2‚Üí2 with Softmax)', fontsize=14, fontweight='bold')\n",
    "\n",
    "# Initialize plots\n",
    "scatter0 = ax1.scatter(X0[:, 0], X0[:, 1], c='red', edgecolors='white', s=60, label='Red (class 0)')\n",
    "scatter1 = ax1.scatter(X1[:, 0], X1[:, 1], c='blue', edgecolors='white', s=60, label='Blue (class 1)')\n",
    "ax1.set_xlim(-2, 2)\n",
    "ax1.set_ylim(-2, 2)\n",
    "ax1.set_xlabel('Feature 1')\n",
    "ax1.set_ylabel('Feature 2')\n",
    "ax1.legend()\n",
    "title = ax1.set_title('Epoch 0')\n",
    "\n",
    "# Loss plot\n",
    "loss_line, = ax2.plot([], [], 'b-', linewidth=2)\n",
    "ax2.set_xlim(0, epochs)\n",
    "ax2.set_ylim(0, max(losses) * 1.1)\n",
    "ax2.set_xlabel('Epoch')\n",
    "ax2.set_ylabel('CrossEntropy Loss')\n",
    "ax2.set_title('Training Loss')\n",
    "ax2.grid(True, alpha=0.3)\n",
    "\n",
    "def init():\n",
    "    return []\n",
    "\n",
    "def update(frame):\n",
    "    # Clear previous contour\n",
    "    for c in ax1.collections[2:]:\n",
    "        c.remove()\n",
    "    \n",
    "    # Draw new decision boundary\n",
    "    ax1.contourf(xx, yy, boundaries[frame], levels=[0, 0.5, 1], \n",
    "\t\t\t\tcolors=['#ffcccc', '#ccccff'], alpha=0.6)\n",
    "    ax1.contour(xx, yy, boundaries[frame], levels=[0.5], \n",
    "\t\t\t\tcolors=['black'], linewidths=2)\n",
    "    \n",
    "    # Re-draw scatter on top\n",
    "    ax1.scatter(X0[:, 0], X0[:, 1], c='red', edgecolors='white', s=60, zorder=10)\n",
    "    ax1.scatter(X1[:, 0], X1[:, 1], c='blue', edgecolors='white', s=60, zorder=10)\n",
    "    \n",
    "    title.set_text(f'Epoch {frame + 1} | Loss: {losses[frame]:.4f}')\n",
    "    \n",
    "    # Update loss plot\n",
    "    loss_line.set_data(range(frame + 1), losses[:frame + 1])\n",
    "    \n",
    "    return []\n",
    "\n",
    "ani = FuncAnimation(fig, update, frames=epochs, init_func=init, \n",
    "                    interval=100, blit=False)\n",
    "plt.tight_layout()\n",
    "HTML(ani.to_jshtml())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b42118e",
   "metadata": {},
   "source": [
    "## What is a Logit?\n",
    "\n",
    "**Logit** = raw output of a neural network *before* applying sigmoid/softmax.\n",
    "\n",
    "```\n",
    "Input ‚Üí Linear Layer ‚Üí [LOGIT] ‚Üí Sigmoid/Softmax ‚Üí Probability\n",
    "                          ‚Üë\n",
    "                    Range: -‚àû to +‚àû\n",
    "```\n",
    "\n",
    "### Why is it called \"Logit\"?\n",
    "\n",
    "The name comes from **\"log\"** + **\"unit\"** (or logistic unit).\n",
    "\n",
    "It's the **log-odds** (logarithm of the odds ratio):\n",
    "\n",
    "$$\\text{logit}(p) = \\log\\left(\\frac{p}{1-p}\\right)$$\n",
    "\n",
    "Where $p$ is a probability and $\\frac{p}{1-p}$ is the **odds**.\n",
    "\n",
    "| Probability | Odds (p / 1-p) | Logit (log-odds) |\n",
    "|-------------|----------------|------------------|\n",
    "| 0.5 | 1:1 = 1.0 | 0 |\n",
    "| 0.73 | ~2.7:1 = 2.7 | ~1.0 |\n",
    "| 0.95 | 19:1 = 19 | ~2.9 |\n",
    "| 0.01 | 1:99 = 0.01 | -4.6 |\n",
    "\n",
    "### Converting Between Logit and Probability\n",
    "\n",
    "**Logit ‚Üí Probability** (Sigmoid function):\n",
    "\n",
    "$$P = \\frac{1}{1 + e^{-\\text{logit}}}$$\n",
    "\n",
    "| Logit | ‚Üí Probability |\n",
    "|-------|---------------|\n",
    "| -3 | 0.05 |\n",
    "| 0 | 0.50 |\n",
    "| +3 | 0.95 |\n",
    "\n",
    "**Probability ‚Üí Logit** (Logit function):\n",
    "\n",
    "$$\\text{logit} = \\log\\left(\\frac{P}{1-P}\\right)$$\n",
    "\n",
    "These are inverse functions of each other.\n",
    "\n",
    "### Why use logits in neural networks?\n",
    "\n",
    "1. **Numerical stability** ‚Äî probabilities near 0 or 1 cause issues; logits don't\n",
    "2. **CrossEntropyLoss expects logits** ‚Äî applies softmax internally for stability\n",
    "3. **Unbounded range** ‚Äî easier for gradient descent to optimize\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c582078",
   "metadata": {},
   "source": [
    "## From Sigmoid to Softmax (The Natural Progression)\n",
    "\n",
    "### Binary Classification (2 classes)\n",
    "\n",
    "With **1 output** neuron, we use **sigmoid**:\n",
    "\n",
    "$$P(\\text{blue}) = \\frac{1}{1 + e^{-z}}$$\n",
    "\n",
    "where $z$ is the single logit. Then $P(\\text{red}) = 1 - P(\\text{blue})$.\n",
    "\n",
    "---\n",
    "\n",
    "### Multi-class Classification (3+ classes)\n",
    "\n",
    "What if we have **Red, Blue, Green**? We need 3 outputs.\n",
    "\n",
    "Each class gets its own logit: $z_{\\text{red}}, z_{\\text{blue}}, z_{\\text{green}}$\n",
    "\n",
    "**Problem**: How do we convert 3 logits into 3 probabilities that sum to 1?\n",
    "\n",
    "**Solution**: **Softmax**\n",
    "\n",
    "$$P(\\text{class } i) = \\frac{e^{z_i}}{\\sum_j e^{z_j}}$$\n",
    "\n",
    "**Example**:\n",
    "| Class | Logit ($z$) | $e^z$ | Softmax (probability) |\n",
    "|-------|-------------|-------|----------------------|\n",
    "| Red | 2.0 | 7.4 | 7.4 / 12.5 = **0.59** |\n",
    "| Blue | 1.0 | 2.7 | 2.7 / 12.5 = **0.22** |\n",
    "| Green | 0.5 | 1.6 | 1.6 / 12.5 = **0.13** |\n",
    "| | | **Sum: 12.5** | **Sum: 1.00** |\n",
    "\n",
    "### Softmax = Generalized Sigmoid\n",
    "\n",
    "For **2 classes**, softmax reduces to sigmoid! (try the math)\n",
    "\n",
    "```\n",
    "Softmax with 2 outputs    ‚â°    Sigmoid with 1 output\n",
    "     [z‚ÇÄ, z‚ÇÅ]                        z = z‚ÇÅ - z‚ÇÄ\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## From BCE Loss to Cross-Entropy Loss\n",
    "\n",
    "### Binary Cross-Entropy (BCE) Loss\n",
    "\n",
    "For binary classification (sigmoid output):\n",
    "\n",
    "$$\\text{BCE} = -[y \\cdot \\log(p) + (1-y) \\cdot \\log(1-p)]$$\n",
    "\n",
    "- If true label $y=1$ (blue): Loss = $-\\log(p)$ ‚Üí punish low $p$\n",
    "- If true label $y=0$ (red): Loss = $-\\log(1-p)$ ‚Üí punish high $p$\n",
    "\n",
    "### Cross-Entropy Loss (Multi-class)\n",
    "\n",
    "For multi-class (softmax outputs):\n",
    "\n",
    "$$\\text{CE} = -\\log(p_{\\text{correct class}})$$\n",
    "\n",
    "Just the negative log of the probability assigned to the **true class**.\n",
    "\n",
    "**Example**: True class = Red, model outputs $[0.59, 0.22, 0.13]$\n",
    "$$\\text{Loss} = -\\log(0.59) = 0.53$$\n",
    "\n",
    "### Summary Table\n",
    "\n",
    "| Classes | Output Activation | Loss Function | PyTorch |\n",
    "|---------|------------------|---------------|---------|\n",
    "| 2 (binary) | Sigmoid | BCE Loss | `nn.BCELoss()` |\n",
    "| 2+ (multi) | Softmax | Cross-Entropy | `nn.CrossEntropyLoss()` |\n",
    "\n",
    "**Note**: `nn.CrossEntropyLoss()` combines softmax + CE loss internally for numerical stability. That's why we pass **logits**, not probabilities.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09897962",
   "metadata": {},
   "source": [
    "## Why Exponentiate? Why Not Just Normalize?\n",
    "\n",
    "Great question! You're asking: why use $\\frac{e^{z_i}}{\\sum e^{z_j}}$ instead of just $\\frac{z_i}{\\sum z_j}$?\n",
    "\n",
    "### Problem 1: Logits Can Be Negative\n",
    "\n",
    "Logits range from $-\\infty$ to $+\\infty$. Simple normalization breaks:\n",
    "\n",
    "| Class | Logit | Simple normalize | Problem |\n",
    "|-------|-------|------------------|---------|\n",
    "| Red | 3 | 3/4 = 0.75 | |\n",
    "| Blue | 1 | 1/4 = 0.25 | ‚úì works |\n",
    "\n",
    "| Class | Logit | Simple normalize | Problem |\n",
    "|-------|-------|------------------|---------|\n",
    "| Red | 2 | 2/0 = ??? | √∑ by zero! |\n",
    "| Blue | -2 | -2/0 = ??? | |\n",
    "\n",
    "| Class | Logit | Simple normalize | Problem |\n",
    "|-------|-------|------------------|---------|\n",
    "| Red | -1 | -1/-3 = 0.33 | |\n",
    "| Blue | -2 | -2/-3 = 0.67 | ‚ùå Blue wins but has LOWER score! |\n",
    "\n",
    "**Exponential fixes this**: $e^z > 0$ always, so no negatives or zeros.\n",
    "\n",
    "### Problem 2: We Want to Amplify Differences\n",
    "\n",
    "Softmax makes the **largest logit dominate**:\n",
    "\n",
    "| Class | Logit | $e^z$ | Softmax |\n",
    "|-------|-------|-------|---------|\n",
    "| Red | 5 | 148.4 | **0.88** |\n",
    "| Blue | 3 | 20.1 | 0.12 |\n",
    "\n",
    "The difference was only 2, but softmax gives Red 88% confidence.\n",
    "\n",
    "With simple average: Red = 5/8 = 0.625. Much less decisive.\n",
    "\n",
    "### Problem 3: Mathematical Properties\n",
    "\n",
    "1. **Gradients are clean**: $\\frac{\\partial}{\\partial z_i} \\text{softmax}(z)_i = p_i(1-p_i)$ (same form as sigmoid!)\n",
    "2. **Connects to physics**: Boltzmann distribution in thermodynamics\n",
    "3. **Information theory**: Minimizing cross-entropy = maximizing likelihood\n",
    "\n",
    "### Your Intuition Isn't Wrong Though!\n",
    "\n",
    "There ARE alternatives:\n",
    "- **Sparsemax**: Can output exact zeros (sparse probabilities)\n",
    "- **Temperature scaling**: $\\frac{e^{z_i / T}}{\\sum e^{z_j / T}}$ controls \"sharpness\"\n",
    "  - $T \\to 0$: Winner takes all (argmax)\n",
    "  - $T \\to \\infty$: Uniform distribution (your averaging idea!)\n",
    "  - $T = 1$: Standard softmax\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fad3b2e",
   "metadata": {},
   "source": [
    "## Multi-Layer Neural Network\n",
    "\n",
    "```\n",
    "Input (2)  ‚Üí  Hidden1 (16 neurons)  ‚Üí  Hidden2 (16 neurons)  ‚Üí  Output (2)\n",
    "   x‚ÇÅ ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚Üí [16 neurons] ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚Üí [16 neurons] ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚Üí Red\n",
    "   x‚ÇÇ ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò        + ReLU                 + ReLU           ‚îî‚îÄ‚îÄ‚Üí Blue\n",
    "```\n",
    "\n",
    "**Why 16 neurons?** More neurons = more capacity to learn complex curved boundaries.\n",
    "\n",
    "With only 2-3 neurons, the network can only combine a few \"directions\" ‚Üí still mostly linear. With 16, it can carve out circular/complex shapes.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b75c23c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.animation import FuncAnimation\n",
    "from IPython.display import HTML, display\n",
    "import ipywidgets as widgets\n",
    "\n",
    "def train_and_animate(layer1_neurons, layer2_neurons, seed, epochs):\n",
    "    \"\"\"Train a network with custom architecture and show animation.\"\"\"\n",
    "    \n",
    "    # Set random seeds\n",
    "    torch.manual_seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    \n",
    "    # Generate circular data\n",
    "    n_points = 150\n",
    "    \n",
    "    # Class 0 (Red): Inner circle\n",
    "    theta0 = np.random.uniform(0, 2*np.pi, n_points)\n",
    "    r0 = np.random.uniform(0, 0.5, n_points)\n",
    "    X0 = np.column_stack([r0 * np.cos(theta0), r0 * np.sin(theta0)])\n",
    "    \n",
    "    # Class 1 (Blue): Outer ring\n",
    "    theta1 = np.random.uniform(0, 2*np.pi, n_points)\n",
    "    r1 = np.random.uniform(0.7, 1.0, n_points)\n",
    "    X1 = np.column_stack([r1 * np.cos(theta1), r1 * np.sin(theta1)])\n",
    "    \n",
    "    X = np.vstack([X0, X1]).astype(np.float32)\n",
    "    y = np.array([0]*n_points + [1]*n_points)\n",
    "    \n",
    "    X_tensor = torch.from_numpy(X)\n",
    "    y_tensor = torch.from_numpy(y).long()\n",
    "    \n",
    "    # Build network with custom architecture\n",
    "    class CustomClassifier(nn.Module):\n",
    "        def __init__(self, n1, n2):\n",
    "            super().__init__()\n",
    "            self.layer1 = nn.Linear(2, n1)\n",
    "            self.layer2 = nn.Linear(n1, n2)\n",
    "            self.layer3 = nn.Linear(n2, 2)\n",
    "            self.relu = nn.ReLU()\n",
    "        \n",
    "        def forward(self, x):\n",
    "            x = self.relu(self.layer1(x))\n",
    "            x = self.relu(self.layer2(x))\n",
    "            x = self.layer3(x)\n",
    "            return x\n",
    "    \n",
    "    model = CustomClassifier(layer1_neurons, layer2_neurons)\n",
    "    \n",
    "    # Count parameters\n",
    "    total_params = sum(p.numel() for p in model.parameters())\n",
    "    print(f\"Architecture: 2 ‚Üí {layer1_neurons} ‚Üí {layer2_neurons} ‚Üí 2\")\n",
    "    print(f\"Total parameters: {total_params}\")\n",
    "    \n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=0.05)\n",
    "    \n",
    "    # Training\n",
    "    boundaries = []\n",
    "    losses = []\n",
    "    \n",
    "    # Reduced grid resolution for faster animation (50x50 instead of 100x100)\n",
    "    xx, yy = np.meshgrid(np.linspace(-1.5, 1.5, 50), np.linspace(-1.5, 1.5, 50))\n",
    "    grid = torch.from_numpy(np.c_[xx.ravel(), yy.ravel()].astype(np.float32))\n",
    "    \n",
    "    # Progress display\n",
    "    progress = widgets.IntProgress(value=0, min=0, max=epochs, description='Training:')\n",
    "    status = widgets.Label(value='')\n",
    "    display(widgets.HBox([progress, status]))\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        outputs = model(X_tensor)\n",
    "        loss = criterion(outputs, y_tensor)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        # Update progress\n",
    "        progress.value = epoch + 1\n",
    "        status.value = f'Epoch {epoch + 1}/{epochs} | Loss: {loss.item():.4f}'\n",
    "        \n",
    "        if epoch % 2 == 0:\n",
    "            with torch.no_grad():\n",
    "                logits = model(grid)\n",
    "                probs = torch.softmax(logits, dim=1)\n",
    "                Z = probs[:, 1].numpy().reshape(xx.shape)\n",
    "                boundaries.append(Z.copy())\n",
    "                losses.append(loss.item())\n",
    "    \n",
    "    status.value = f'Done! Final Loss: {losses[-1]:.4f}'\n",
    "    \n",
    "    # Create animation\n",
    "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 5))\n",
    "    fig.suptitle(f'Network: 2 ‚Üí {layer1_neurons} ‚Üí {layer2_neurons} ‚Üí 2  ({total_params} params)', \n",
    "\t\t\t\tfontsize=14, fontweight='bold')\n",
    "    \n",
    "    ax1.scatter(X0[:, 0], X0[:, 1], c='red', edgecolors='white', s=40, label='Red')\n",
    "    ax1.scatter(X1[:, 0], X1[:, 1], c='blue', edgecolors='white', s=40, label='Blue')\n",
    "    ax1.set_xlim(-1.5, 1.5)\n",
    "    ax1.set_ylim(-1.5, 1.5)\n",
    "    ax1.set_aspect('equal')\n",
    "    ax1.legend(loc='upper right')\n",
    "    title = ax1.set_title('Epoch 0')\n",
    "    \n",
    "    loss_line, = ax2.plot([], [], 'b-', linewidth=2)\n",
    "    ax2.set_xlim(0, len(boundaries))\n",
    "    ax2.set_ylim(0, max(losses) * 1.1)\n",
    "    ax2.set_xlabel('Epoch (√∑2)')\n",
    "    ax2.set_ylabel('Loss')\n",
    "    ax2.grid(True, alpha=0.3)\n",
    "    \n",
    "    def update(frame):\n",
    "        for c in ax1.collections[2:]:\n",
    "            c.remove()\n",
    "        \n",
    "        ax1.contourf(xx, yy, boundaries[frame], levels=np.linspace(0, 1, 20), \n",
    "\t\t\t\t\tcmap='RdBu', alpha=0.6)\n",
    "        ax1.contour(xx, yy, boundaries[frame], levels=[0.5], colors=['black'], linewidths=2)\n",
    "        \n",
    "        ax1.scatter(X0[:, 0], X0[:, 1], c='red', edgecolors='white', s=40, zorder=10)\n",
    "        ax1.scatter(X1[:, 0], X1[:, 1], c='blue', edgecolors='white', s=40, zorder=10)\n",
    "        \n",
    "        title.set_text(f'Epoch {frame * 2} | Loss: {losses[frame]:.4f}')\n",
    "        loss_line.set_data(range(frame + 1), losses[:frame + 1])\n",
    "        return []\n",
    "    \n",
    "    ani = FuncAnimation(fig, update, frames=len(boundaries), interval=80, blit=False)\n",
    "    plt.tight_layout()\n",
    "    plt.close(fig)\n",
    "    return HTML(ani.to_jshtml())\n",
    "\n",
    "# Create input widgets\n",
    "layer1_input = widgets.IntSlider(value=8, min=2, max=32, step=1, description='Layer 1:')\n",
    "layer2_input = widgets.IntSlider(value=8, min=2, max=32, step=1, description='Layer 2:')\n",
    "seed_input = widgets.IntText(value=0, description='Seed:')\n",
    "epochs_input = widgets.IntSlider(value=50, min=10, max=300, step=10, description='Epochs:')\n",
    "run_button = widgets.Button(description='Train & Animate', button_style='primary')\n",
    "output = widgets.Output()\n",
    "\n",
    "def on_button_click(b):\n",
    "    output.clear_output()\n",
    "    with output:\n",
    "        print(f\"üé≤ Seed: {seed_input.value} | Epochs: {epochs_input.value}\")\n",
    "        result = train_and_animate(layer1_input.value, layer2_input.value, \n",
    "                                   seed_input.value, epochs_input.value)\n",
    "        display(result)\n",
    "\n",
    "run_button.on_click(on_button_click)\n",
    "\n",
    "# Display widgets\n",
    "display(widgets.VBox([\n",
    "    widgets.HBox([layer1_input, layer2_input]),\n",
    "    widgets.HBox([seed_input, epochs_input]),\n",
    "    run_button,\n",
    "    output\n",
    "]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d4dbcc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import ipywidgets as widgets\n",
    "from IPython.display import display\n",
    "\n",
    "def test_architecture_with_decay(n1, n2, num_seeds=20, epochs=300):\n",
    "    \"\"\"Test architecture and track alive neurons per epoch.\"\"\"\n",
    "    all_results = []\n",
    "    \n",
    "    for seed in range(num_seeds):\n",
    "        torch.manual_seed(seed)\n",
    "        np.random.seed(seed)\n",
    "        \n",
    "        # Generate circular data\n",
    "        n_points = 150\n",
    "        theta0 = np.random.uniform(0, 2*np.pi, n_points)\n",
    "        r0 = np.random.uniform(0, 0.5, n_points)\n",
    "        X0 = np.column_stack([r0 * np.cos(theta0), r0 * np.sin(theta0)])\n",
    "        \n",
    "        theta1 = np.random.uniform(0, 2*np.pi, n_points)\n",
    "        r1 = np.random.uniform(0.7, 1.0, n_points)\n",
    "        X1 = np.column_stack([r1 * np.cos(theta1), r1 * np.sin(theta1)])\n",
    "        \n",
    "        X = torch.from_numpy(np.vstack([X0, X1]).astype(np.float32))\n",
    "        y = torch.from_numpy(np.array([0]*n_points + [1]*n_points)).long()\n",
    "        \n",
    "        # Model with activation tracking\n",
    "        class TrackedNet(nn.Module):\n",
    "            def __init__(self, n1, n2):\n",
    "                super().__init__()\n",
    "                self.l1 = nn.Linear(2, n1)\n",
    "                self.l2 = nn.Linear(n1, n2)\n",
    "                self.l3 = nn.Linear(n2, 2)\n",
    "                self.act1 = None\n",
    "                self.act2 = None\n",
    "            \n",
    "            def forward(self, x):\n",
    "                x = torch.relu(self.l1(x))\n",
    "                self.act1 = x.detach()\n",
    "                x = torch.relu(self.l2(x))\n",
    "                self.act2 = x.detach()\n",
    "                return self.l3(x)\n",
    "        \n",
    "        model = TrackedNet(n1, n2)\n",
    "        optimizer = torch.optim.Adam(model.parameters(), lr=0.05)\n",
    "        criterion = nn.CrossEntropyLoss()\n",
    "        \n",
    "        # Track alive neurons PER LAYER over epochs\n",
    "        alive_l1_history = []\n",
    "        alive_l2_history = []\n",
    "        \n",
    "        for epoch in range(epochs):\n",
    "            out = model(X)\n",
    "            loss = criterion(out, y)\n",
    "            \n",
    "            # Count alive neurons per layer\n",
    "            alive1 = (model.act1.sum(dim=0) > 0).sum().item()\n",
    "            alive2 = (model.act2.sum(dim=0) > 0).sum().item()\n",
    "            alive_l1_history.append(alive1)\n",
    "            alive_l2_history.append(alive2)\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "        \n",
    "        # Final accuracy\n",
    "        with torch.no_grad():\n",
    "            preds = model(X).argmax(dim=1)\n",
    "            acc = (preds == y).float().mean().item()\n",
    "        \n",
    "        all_results.append({\n",
    "            'seed': seed,\n",
    "            'accuracy': acc,\n",
    "            'loss': loss.item(),\n",
    "            'alive_l1': alive_l1_history,\n",
    "            'alive_l2': alive_l2_history,\n",
    "            'final_alive': alive_l1_history[-1] + alive_l2_history[-1]\n",
    "        })\n",
    "    \n",
    "    return all_results, n1, n2\n",
    "\n",
    "# Widgets\n",
    "layer1_test = widgets.IntSlider(value=4, min=2, max=32, step=1, description='Layer 1:')\n",
    "layer2_test = widgets.IntSlider(value=4, min=2, max=32, step=1, description='Layer 2:')\n",
    "seeds_test = widgets.IntSlider(value=20, min=5, max=50, step=5, description='# Seeds:')\n",
    "test_button = widgets.Button(description='Test Success Rate', button_style='success')\n",
    "test_output = widgets.Output()\n",
    "\n",
    "def on_test_click(b):\n",
    "    test_output.clear_output()\n",
    "    with test_output:\n",
    "        n1, n2 = layer1_test.value, layer2_test.value\n",
    "        num_seeds = seeds_test.value\n",
    "        total_neurons = n1 + n2\n",
    "        \n",
    "        print(f\"Testing 2 ‚Üí {n1} ‚Üí {n2} ‚Üí 2 with {num_seeds} seeds...\")\n",
    "        print(\"=\" * 50)\n",
    "        \n",
    "        results, _, _ = test_architecture_with_decay(n1, n2, num_seeds)\n",
    "        \n",
    "        successes = sum(1 for r in results if r['accuracy'] > 0.95)\n",
    "        rate = successes / num_seeds * 100\n",
    "        \n",
    "        # Color-coded result\n",
    "        if rate >= 80:\n",
    "            emoji = \"üü¢\"\n",
    "        elif rate >= 50:\n",
    "            emoji = \"üü°\"\n",
    "        else:\n",
    "            emoji = \"üî¥\"\n",
    "        \n",
    "        print(f\"\\n{emoji} Success Rate: {successes}/{num_seeds} = {rate:.0f}%\")\n",
    "        \n",
    "        params = 2*n1 + n1 + n1*n2 + n2 + n2*2 + 2\n",
    "        print(f\"   Total Parameters: {params}\")\n",
    "        print(f\"   Total Neurons: {total_neurons}\")\n",
    "        \n",
    "        # Plot: Grid of graphs - one per seed (BIGGER - 2 per row)\n",
    "        cols = 2\n",
    "        rows = (num_seeds + cols - 1) // cols\n",
    "        fig, axes = plt.subplots(rows, cols, figsize=(14, 5 * rows))\n",
    "        axes = axes.flatten()\n",
    "        \n",
    "        for i, r in enumerate(results):\n",
    "            ax = axes[i]\n",
    "            \n",
    "            # Color based on success/fail\n",
    "            if r['accuracy'] > 0.95:\n",
    "                color = 'green'\n",
    "                status = '‚úÖ SUCCESS'\n",
    "            else:\n",
    "                color = 'red'\n",
    "                status = '‚ùå FAILED'\n",
    "            \n",
    "            # Plot alive neurons PER LAYER\n",
    "            epochs_range = range(len(r['alive_l1']))\n",
    "            \n",
    "            # Layer 1 line\n",
    "            ax.plot(epochs_range, r['alive_l1'], color='blue', linewidth=2.5, \n",
    "                   label=f'Layer 1 (max {n1})')\n",
    "            ax.axhline(y=n1, color='blue', linestyle='--', alpha=0.3, linewidth=1)\n",
    "            \n",
    "            # Layer 2 line\n",
    "            ax.plot(epochs_range, r['alive_l2'], color='orange', linewidth=2.5, \n",
    "                   label=f'Layer 2 (max {n2})')\n",
    "            ax.axhline(y=n2, color='orange', linestyle='--', alpha=0.3, linewidth=1)\n",
    "            \n",
    "            # Mark final values\n",
    "            end_l1 = r['alive_l1'][-1]\n",
    "            end_l2 = r['alive_l2'][-1]\n",
    "            \n",
    "            ax.annotate(f'L1: {end_l1}/{n1}', xy=(290, end_l1), fontsize=9, color='blue',\n",
    "                       fontweight='bold', ha='right')\n",
    "            ax.annotate(f'L2: {end_l2}/{n2}', xy=(290, end_l2 - 0.8), fontsize=9, color='orange',\n",
    "                       fontweight='bold', ha='right')\n",
    "            \n",
    "            # Background color based on success/fail\n",
    "            ax.set_facecolor('#e8f5e9' if r['accuracy'] > 0.95 else '#ffebee')\n",
    "            \n",
    "            ax.set_ylim(0, max(n1, n2) + 1)\n",
    "            ax.set_xlim(0, 300)\n",
    "            ax.set_title(f\"Seed {r['seed']} ‚Äî {status}\\nAccuracy: {r['accuracy']*100:.0f}%\", \n",
    "                        fontsize=11, fontweight='bold', color=color)\n",
    "            ax.set_xlabel('Epoch', fontsize=10)\n",
    "            ax.set_ylabel('Alive Neurons', fontsize=10)\n",
    "            ax.grid(True, alpha=0.3)\n",
    "            ax.legend(loc='lower left', fontsize=8)\n",
    "        \n",
    "        # Hide empty subplots\n",
    "        for i in range(num_seeds, len(axes)):\n",
    "            axes[i].axis('off')\n",
    "        \n",
    "        fig.suptitle(f'How Many Neurons Stay Alive During Training?\\n'\n",
    "                     f'Architecture: 2 ‚Üí {n1} ‚Üí {n2} ‚Üí 2 (Total: {total_neurons} neurons)\\n'\n",
    "                     f'Line going DOWN = neurons dying = network losing capacity', \n",
    "                     fontsize=14, fontweight='bold')\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "        \n",
    "        # Add explanation\n",
    "        print(\"\\n\" + \"=\"*60)\n",
    "        print(\"üìñ HOW TO READ THESE GRAPHS:\")\n",
    "        print(\"=\"*60)\n",
    "        print(f\"‚Ä¢ üîµ Blue line = Layer 1 alive neurons (max {n1})\")\n",
    "        print(f\"‚Ä¢ üü† Orange line = Layer 2 alive neurons (max {n2})\")\n",
    "        print(f\"‚Ä¢ Dashed lines = Maximum for each layer\")\n",
    "        print(f\"‚Ä¢ Green background = SUCCESS | Red background = FAILED\")\n",
    "        print(f\"‚Ä¢ Line DROPS ‚Üí neurons died ‚Üí that layer lost capacity\")\n",
    "        print(f\"‚Ä¢ If BOTH layers lose neurons ‚Üí network can't learn!\")\n",
    "        \n",
    "        # Print summary\n",
    "        print(f\"\\nüìä Neuron Survival Summary:\")\n",
    "        dead_runs = [r for r in results if r['final_alive'] < total_neurons * 0.5]\n",
    "        print(f\"   Runs with >50% dead neurons: {len(dead_runs)}/{num_seeds}\")\n",
    "        \n",
    "        failed = [r for r in results if r['accuracy'] <= 0.95]\n",
    "        if failed:\n",
    "            avg_alive_failed = np.mean([r['final_alive'] for r in failed])\n",
    "            print(f\"   Avg alive neurons in FAILED runs: {avg_alive_failed:.1f}/{total_neurons}\")\n",
    "        \n",
    "        succeeded = [r for r in results if r['accuracy'] > 0.95]\n",
    "        if succeeded:\n",
    "            avg_alive_success = np.mean([r['final_alive'] for r in succeeded])\n",
    "            print(f\"   Avg alive neurons in SUCCESS runs: {avg_alive_success:.1f}/{total_neurons}\")\n",
    "        \n",
    "        # Only show best/worst if there's variance\n",
    "        if rate == 100:\n",
    "            print(f\"\\nüéâ All seeds succeeded! Most neurons alive:\")\n",
    "            for r in sorted(results, key=lambda x: -x['final_alive'])[:3]:\n",
    "                print(f\"   Seed {r['seed']}: {r['final_alive']}/{total_neurons} alive\")\n",
    "        elif rate == 0:\n",
    "            print(f\"\\nüíÄ All seeds failed! Least dead neurons:\")\n",
    "            for r in sorted(results, key=lambda x: -x['final_alive'])[:3]:\n",
    "                print(f\"   Seed {r['seed']}: {r['final_alive']}/{total_neurons} alive, {r['accuracy']*100:.1f}% acc\")\n",
    "        else:\n",
    "            print(f\"\\n‚úÖ Best seeds (succeeded):\")\n",
    "            for r in sorted([r for r in results if r['accuracy'] > 0.95], key=lambda x: -x['final_alive'])[:3]:\n",
    "                print(f\"   Seed {r['seed']}: {r['final_alive']}/{total_neurons} alive\")\n",
    "            \n",
    "            print(f\"\\n‚ùå Worst seeds (failed):\")\n",
    "            for r in sorted([r for r in results if r['accuracy'] <= 0.95], key=lambda x: x['accuracy'])[:3]:\n",
    "                print(f\"   Seed {r['seed']}: {r['accuracy']*100:.1f}% acc, {r['final_alive']}/{total_neurons} alive\")\n",
    "\n",
    "test_button.on_click(on_test_click)\n",
    "\n",
    "display(widgets.VBox([\n",
    "    widgets.HTML(\"<h3>üß™ Test Architecture Success Rate</h3>\"),\n",
    "    widgets.HBox([layer1_test, layer2_test, seeds_test]),\n",
    "    test_button,\n",
    "    test_output\n",
    "]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c76d4374",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "d277a4ef",
   "metadata": {},
   "source": [
    "## Why Do Some Architectures Work and Others Fail?\n",
    "\n",
    "Great observation! You noticed:\n",
    "- 4,4 fails | 4,3 works\n",
    "- 3,6 fails | 3,7 works  \n",
    "- 5,3 works\n",
    "\n",
    "### It's Not Exactly Polynomials ‚Äî It's **Piecewise Linear Regions**\n",
    "\n",
    "With **ReLU activation**, neural networks don't create smooth polynomial curves. Instead, they create **piecewise linear** decision boundaries (like a polygon approximating a circle).\n",
    "\n",
    "```\n",
    "Smooth circle:  ‚óã        ReLU network:  ‚¨° (polygon with flat edges)\n",
    "```\n",
    "\n",
    "### How Many \"Sides\" Can a Network Create?\n",
    "\n",
    "Each ReLU neuron creates a **hyperplane** (a line in 2D). The network combines these to carve up space:\n",
    "\n",
    "| Layer 1 neurons | Creates | Effect |\n",
    "|----------------|---------|--------|\n",
    "| 1 | 1 line | Splits space in 2 |\n",
    "| 2 | 2 lines | Up to 4 regions |\n",
    "| 3 | 3 lines | Up to 7 regions |\n",
    "| n | n lines | Up to ~n¬≤ regions |\n",
    "\n",
    "**Maximum linear regions** for a ReLU network with layers of width $n_1, n_2, ..., n_L$:\n",
    "\n",
    "$$\\text{Regions} \\leq \\prod_{i=1}^{L} \\sum_{j=0}^{\\min(n_i, d)} \\binom{n_i}{j}$$\n",
    "\n",
    "For practical purposes: **more neurons ‚âà more \"polygon sides\" ‚âà smoother curves**\n",
    "\n",
    "### Why Some Fail Despite Having \"Enough\" Neurons?\n",
    "\n",
    "1. **Random initialization** ‚Äî Some starting weights land in bad spots\n",
    "2. **Optimization landscape** ‚Äî Gradient descent can get stuck\n",
    "3. **First layer width matters most** ‚Äî It does the initial \"space transformation\"\n",
    "\n",
    "Your observations hint at this: it's not just total neurons, but **how they're distributed**.\n",
    "\n",
    "### Try This Experiment\n",
    "\n",
    "| Architecture | Total Params | Linear Regions (rough) | Works? |\n",
    "|-------------|--------------|------------------------|--------|\n",
    "| 2 ‚Üí 4 ‚Üí 4 ‚Üí 2 | 42 | ~16-64 | ‚ùì |\n",
    "| 2 ‚Üí 8 ‚Üí 2 ‚Üí 2 | 36 | ~8-16 | ‚ùì |\n",
    "| 2 ‚Üí 4 ‚Üí 8 ‚Üí 2 | 58 | ~32-128 | ‚ùì |\n",
    "\n",
    "The **depth vs width** tradeoff: wider first layers help, but deeper networks can compose more complex functions.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86b6af22",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "969132b4",
   "metadata": {},
   "source": [
    "## Key Insight: 4,4 IS Capable ‚Äî But Gets Stuck!\n",
    "\n",
    "You just discovered one of the classic problems in deep learning! üéØ\n",
    "\n",
    "| Result | Meaning |\n",
    "|--------|---------|\n",
    "| 45% success | Architecture CAN learn it |\n",
    "| 100% when works | Perfectly learns the circle |\n",
    "| 50% when fails | Predicts ONE class for everything |\n",
    "| Loss = 0.6931 | Exactly $-\\ln(0.5)$ = stuck at \"random guessing\" |\n",
    "\n",
    "### What's Happening?\n",
    "\n",
    "```\n",
    "Good initialization:          Bad initialization:\n",
    "   ‚Üò                             ‚Üò\n",
    "    ‚Üí finds solution ‚úì            ‚Üí stuck in flat region ‚úó\n",
    "                                     (gradients ‚âà 0)\n",
    "```\n",
    "\n",
    "This is the **\"dead ReLU\" problem** or **saddle point problem**:\n",
    "\n",
    "1. Some initial weights cause ReLU neurons to output 0 for all inputs\n",
    "2. If all neurons are \"dead\", gradients are 0 ‚Üí no learning\n",
    "3. The network predicts ~50% (random) and can't escape\n",
    "\n",
    "### Why Does 8,8 Work More Reliably?\n",
    "\n",
    "More neurons = **redundancy**:\n",
    "- If 2 out of 8 neurons die, 6 still work\n",
    "- If 2 out of 4 neurons die, only 2 left ‚Üí might not be enough\n",
    "\n",
    "### Solutions to This Problem:\n",
    "\n",
    "1. **Use more neurons** (redundancy)\n",
    "2. **Try different random seeds** (you found this!)\n",
    "3. **Use LeakyReLU** instead of ReLU (no dead neurons)\n",
    "4. **Better initialization** (He initialization, etc.)\n",
    "5. **Use batch normalization**\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.14.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
